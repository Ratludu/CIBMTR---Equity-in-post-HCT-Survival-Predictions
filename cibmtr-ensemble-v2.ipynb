{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":70942,"databundleVersionId":10381525,"sourceType":"competition"},{"sourceId":211253469,"sourceType":"kernelVersion"},{"sourceId":211322530,"sourceType":"kernelVersion"}],"dockerImageVersionId":30804,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-04T06:01:14.737805Z","iopub.execute_input":"2025-01-04T06:01:14.738620Z","iopub.status.idle":"2025-01-04T06:01:14.748157Z","shell.execute_reply.started":"2025-01-04T06:01:14.738584Z","shell.execute_reply":"2025-01-04T06:01:14.747302Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install /kaggle/input/pip-install-lifelines/autograd-1.7.0-py3-none-any.whl\n!pip install /kaggle/input/pip-install-lifelines/autograd-gamma-0.5.0.tar.gz\n!pip install /kaggle/input/pip-install-lifelines/interface_meta-1.3.0-py3-none-any.whl\n!pip install /kaggle/input/pip-install-lifelines/formulaic-1.0.2-py3-none-any.whl\n!pip install /kaggle/input/pip-install-lifelines/lifelines-0.30.0-py3-none-any.whl\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T06:01:14.749571Z","iopub.execute_input":"2025-01-04T06:01:14.749858Z","iopub.status.idle":"2025-01-04T06:04:40.835955Z","shell.execute_reply.started":"2025-01-04T06:01:14.749829Z","shell.execute_reply":"2025-01-04T06:04:40.834817Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from lifelines import KaplanMeierFitter, CoxPHFitter, NelsonAalenFitter\nfrom metric import score\nfrom scipy.stats import rankdata \nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBRegressor, XGBClassifier\nimport xgboost as xgb\nfrom lightgbm import LGBMRegressor,LGBMClassifier\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nimport catboost as cb\nimport seaborn as sns\nfrom sklearn.preprocessing import OneHotEncoder\nfrom pathlib import Path\nimport optuna","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T06:04:40.837299Z","iopub.execute_input":"2025-01-04T06:04:40.837592Z","iopub.status.idle":"2025-01-04T06:04:40.844287Z","shell.execute_reply.started":"2025-01-04T06:04:40.837567Z","shell.execute_reply":"2025-01-04T06:04:40.843293Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def stratified_c_index(df, predictions):\n    y_true = df[[\"ID\", \"efs\", \"efs_time\",\"race_group\"]].copy()\n    y_pred = df[[\"ID\"]].copy()\n    y_pred[\"prediction\"] = predictions\n    m = score(y_true.copy(), y_pred.copy(),\"ID\")\n    return m","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T06:04:40.846328Z","iopub.execute_input":"2025-01-04T06:04:40.846638Z","iopub.status.idle":"2025-01-04T06:04:40.858164Z","shell.execute_reply.started":"2025-01-04T06:04:40.846595Z","shell.execute_reply":"2025-01-04T06:04:40.857415Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class settings:\n\n    train_link = \"/kaggle/input/equity-post-HCT-survival-predictions/train.csv\"\n    test_link =  \"/kaggle/input/equity-post-HCT-survival-predictions/test.csv\"\n    sub_link = \"/kaggle/input/equity-post-HCT-survival-predictions/sample_submission.csv\"\n    \n    seed = 42\n    target = 'target'\n    n_splits = 10\n    col_ignore = ['efs_time','efs','target','ID']\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T06:04:40.859362Z","iopub.execute_input":"2025-01-04T06:04:40.859671Z","iopub.status.idle":"2025-01-04T06:04:40.871312Z","shell.execute_reply.started":"2025-01-04T06:04:40.859644Z","shell.execute_reply":"2025-01-04T06:04:40.870670Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = pd.read_csv(settings.train_link)\ntest = pd.read_csv(settings.test_link)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T06:04:40.872229Z","iopub.execute_input":"2025-01-04T06:04:40.872537Z","iopub.status.idle":"2025-01-04T06:04:41.150295Z","shell.execute_reply.started":"2025-01-04T06:04:40.872473Z","shell.execute_reply":"2025-01-04T06:04:41.149515Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T06:04:41.151417Z","iopub.execute_input":"2025-01-04T06:04:41.151728Z","iopub.status.idle":"2025-01-04T06:04:41.190975Z","shell.execute_reply.started":"2025-01-04T06:04:41.151698Z","shell.execute_reply":"2025-01-04T06:04:41.190075Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SurvModel:\n\n    def __init__(self, model, n_splits = settings.n_splits, random_state = settings.seed, estimator = 'na'):\n        \"\"\"\n        Initialise the SurvModel class\n\n        Parameters:\n        - model: A model instance\n        - n_splits: Number of folds for k-fold cross validation\n        - random_state: for reproducability \n        \"\"\"\n\n        self.model = model\n        self.n_splits = n_splits\n        self.random_state = random_state\n        self.estimator = estimator\n        self.oof = None\n        self.pred = None\n        self.scores = []\n        self.cats = []\n\n    def preprocessing(self, train, test):\n        \"\"\"\n        Proprocessing the test and train data\n\n        Parameters\n        - train: training dataw\n        - test: testing data\n        \"\"\"\n        # For now for testing lets use KM estmator\n        # Fitted a survival curve to the training data to predict off\n        \n        if self.estimator == 'km':\n            kmf = KaplanMeierFitter()\n            kmf.fit(durations = train['efs_time'], event_observed = train['efs'])\n            train[settings.target] = kmf.survival_function_at_times(train['efs_time']).values\n            train[settings.target] = np.log(train[settings.target])\n            print(\"#\"*25,\"KM Estmator Fitted\", \"#\"*25)\n\n        elif self.estimator == 'na':\n            naf = NelsonAalenFitter()\n            naf.fit(durations = train['efs_time'], event_observed = train['efs'])\n            train[settings.target] = -naf.cumulative_hazard_at_times(train['efs_time']).values\n            print(\"#\"*25,\"Nelson-Aalen Estmator Fitted\", \"#\"*25)\n\n        else:\n            train[settings.target] = train.efs_time.copy()\n            train.loc[train.efs==0,settings.target]*=-1\n            print(\"#\"*25,\"CPH Fitted\", \"#\"*25)\n\n        features = [col for col in train.columns if not col in settings.col_ignore]\n        \n        # Combined test and train to label encode\n        combined = pd.concat([train,test], axis = 0, ignore_index = True)\n\n        for col in features:\n            if combined[col].dtype == \"object\":\n                self.cats.append(col)\n                combined[col] = combined[col].fillna(\"NAN\")\n                combined[col],_ = combined[col].factorize()\n                combined[col] -= combined[col].min()\n                combined[col] = combined[col].astype(\"int32\")\n                combined[col] = combined[col].astype(\"category\")\n\n            else:\n                if combined[col].dtype==\"float64\":\n                    combined[col] = combined[col].astype(\"float32\")\n                if combined[col].dtype==\"int64\":\n                    combined[col] = combined[col].astype(\"int32\")\n        train = combined.iloc[:len(train)].copy()\n        test = combined.iloc[len(train):].reset_index(drop=True).copy().drop([settings.target,'efs','efs_time'], axis = 1)\n        return train, test\n\n    def fit_predict(self, train, test, catboost = False):\n        \"\"\"\n        Fit the model using k-fold cross validation and collect oof predictions.\n\n        Note: the train and test data include ID, efs and efs_time as we need it for the metric.\n              We need to remove them for training but add them back in for metric collection.\n        \n        Parameters:\n        - Train: the training data\n        - Test: the test data from kaggle\n\n        Returns:\n        - OOF predictions\n        _ Preditions for the test data\n        \"\"\"\n        # Collect features\n\n        features = [col for col in train.columns if not col in settings.col_ignore]\n        print(\"#\"*25,\"Features\",\"#\"*25)\n        print(f\"There are {len(features)} features.\")\n        print(features)\n\n        # Initialise k-fold cross validation\n        print(\"#\"*25,\"Initialise Cross-Validation\",\"#\"*25)\n\n        kf = KFold(n_splits = self.n_splits, shuffle=True, random_state=self.random_state)\n\n        # Predictions for OOF and test\n        self.oof = np.zeros(len(train))\n        self.preds = np.zeros(len(test))\n\n        # Cross-validation loop\n        for fold, (train_idx, test_idx) in enumerate(kf.split(train)):\n            \n            print(\"#\"*25,f\"Fold {fold+1}\",\"#\"*25)\n            # Split data\n            x_train, x_val = train.loc[train_idx, features].copy(), train.loc[test_idx, features].copy()\n            y_train, y_val = train.loc[train_idx, settings.target].copy(), train.loc[test_idx, settings.target].copy()\n            x_test = test[features]\n\n            x_score = train.loc[test_idx,[\"ID\", \"efs\", \"efs_time\",\"race_group\"]].copy()\n            \n            # Train on training set\n            if catboost:\n                self.model.fit(x_train, y_train, self.cats, verbose = 0, eval_set=[(x_val,y_val)])\n            else:\n                self.model.fit(x_train, y_train)\n\n            # Predict on validation set\n            val_preds = self.model.predict(x_val)\n            self.oof[test_idx] = val_preds\n\n            # Calculate score\n            m = stratified_c_index(x_score, val_preds)\n            self.scores.append(m)\n            print(f\"Fold {fold+1}: Stratified C-index: {m}\")\n\n            self.preds += self.model.predict(x_test)/self.n_splits\n            \n        train_score = train[[\"ID\", \"efs\", \"efs_time\",\"race_group\"]].copy()\n        print(\"#\"*25,\"Cross-Validation Completed\",\"#\"*25)\n        print(f\"Average Stratified C-Index: {np.mean(self.scores)}\")\n        print(f\"Overall Stratified C-Index: {stratified_c_index(train_score, self.oof)}\")\n        return self.oof, self.preds, np.mean(self.scores)\n\n    def save_predictions(self, filename,train, test):\n        \"\"\"\n        Save predictions in a csv.\n\n        Parameters:\n        - filename: model_name\n        - train: train data for ID\n        - test: test data for ID\n        \"\"\"\n        directory_train = Path(\"/kaggle/working/train\")\n        directory_test = Path(\"/kaggle/working/test\")\n\n        directory_train.mkdir(exist_ok = True)\n        print(f\"Directory '{directory_train}' created successfully\")\n        directory_test.mkdir(exist_ok = True)\n        print(f\"Directory '{directory_test}' created successfully\")\n            \n        \n        # oof train preds\n        train_csv = train.copy()\n        train_csv[\"prediction\"] = self.oof\n\n        train_csv.to_csv(\"train/\"+filename+\"_train.csv\", index = False)\n\n        # test\n        test_csv = test.copy()\n        test_csv[\"prediction\"] = self.preds\n\n        test_csv.to_csv(\"test/\"+filename+\"_test.csv\", index = False)\n\n        print(f\"Out of fold predictions and test predictions have been saved\")\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T06:04:41.192336Z","iopub.execute_input":"2025-01-04T06:04:41.192627Z","iopub.status.idle":"2025-01-04T06:04:41.215469Z","shell.execute_reply.started":"2025-01-04T06:04:41.192599Z","shell.execute_reply":"2025-01-04T06:04:41.214541Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Ensembler:\n    def __init__(self, train_folder, test_folder):\n        \"\"\"\n        Initialse class with the train and test folders.\n\n        Parameters:\n        - train_folder: Path to the train folder containing OOF predictions\n        - test_folder: Path to the predictions\n        \"\"\"\n\n        self.train_folder = Path(train_folder)\n        self.test_folder = Path(test_folder)\n        self.scores = []\n\n    def load_files(self, folder_path):\n        \"\"\"\n        Load the train and test files.\n\n        Returns:\n        - train: consolidated predictions for each dataset\n        - test: consolidated predictions for each dataset\n        \"\"\"\n        predictions = {}\n\n        # loop through the csv files\n        counter = 0\n        for idx, file in enumerate(sorted(folder_path.glob(\"*.csv\"))):\n            try:\n                df = pd.read_csv(file)\n                if \"prediction\" in df.columns:\n                    counter +=1\n                    predictions[f\"prediction_{counter}\"] = df[\"prediction\"].reset_index(drop=True)\n                    print(f\"Loaded 'prediction' column from {file.name}\")\n                else:\n                    print(f\"'prediction' column not found in {file.name}\")\n            except Exception as e:\n                print(f\"Error loading {file.name}: {e}\")\n        if predictions:\n            predictions = pd.DataFrame(predictions)\n            predictions[\"ID\"] = df['ID']\n            return predictions\n        else:\n            print(\"No predictions found.\")\n            return pd.DataFrame()\n\n    def concatenate_train_test(self, train):\n        \"\"\"\n        Concatenate train and test folder data into another train and test set.\n\n        Parameters:\n        - train: we need this to get the target variable in the training set\n\n        Returns\n        - train: Another training set based on the oof predictions.\n        - test: Another test set based on preds.\n        \"\"\"\n        print(f\"Loading predictions from the train folder {self.train_folder}\")\n        train_predictions = self.load_files(self.train_folder)\n        train_predictions[settings.target] = train[settings.target].reset_index(drop=True)\n\n        print(f\"Loading predictions from the test folder {self.test_folder}\")\n        test_predictions = self.load_files(self.test_folder)\n\n        return train_predictions, test_predictions\n\n    def ranking(self, train, test, original_train):\n        \n        # Collect features\n\n        features = [col for col in train.columns if not col in settings.col_ignore]\n        print(\"#\"*25,\"Features\",\"#\"*25)\n        print(f\"There are {len(features)} features.\")\n        print(features)\n\n        # Initialise k-fold cross validation\n        print(\"#\"*25,\"Initialise Cross-Validation\",\"#\"*25)\n\n        kf = KFold(n_splits = settings.n_splits, shuffle=True, random_state=settings.seed)\n\n        # Cross-validation loop\n        for fold, (train_idx, test_idx) in enumerate(kf.split(train)):\n            \n            print(\"#\"*25,f\"Fold {fold+1}\",\"#\"*25)\n            # Split data\n            x_train, x_val = train.loc[train_idx, features].copy(), train.loc[test_idx, features].copy()\n            y_train, y_val = train.loc[train_idx, settings.target].copy(), train.loc[test_idx, settings.target].copy()\n            x_test = test[features]\n\n            x_score = original_train.loc[test_idx,[\"ID\", \"efs\", \"efs_time\",\"race_group\"]].copy()\n\n            val_preds = np.zeros(len(x_val))\n            \n            # Predict on validation set\n            for col in x_val:\n                if col not in settings.col_ignore:\n                    val_preds += rankdata(x_val[col])\n                    \n\n\n            # Calculate score\n            m = stratified_c_index(x_score, val_preds)\n            self.scores.append(m)\n            print(f\"Fold {fold+1}: Stratified C-index: {m}\")\n            \n        overall_pred = np.zeros(len(original_train))\n        for col in train:\n            if col not in settings.col_ignore:\n                overall_pred += rankdata(train[col])\n\n        overall_score = original_train[[\"ID\", \"efs\", \"efs_time\",\"race_group\"]].copy()\n            \n        print(\"#\"*25,\"Cross-Validation Completed\",\"#\"*25)\n        print(f\"Average Stratified C-Index: {np.mean(self.scores)}\")\n        print(f\"Overall Stratified C-Index: {stratified_c_index(overall_score, overall_pred)}\")\n\n        \n\n        \n        \n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T06:04:41.216733Z","iopub.execute_input":"2025-01-04T06:04:41.217053Z","iopub.status.idle":"2025-01-04T06:04:41.233123Z","shell.execute_reply.started":"2025-01-04T06:04:41.217028Z","shell.execute_reply":"2025-01-04T06:04:41.232270Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"estimators = ['km','na','cox']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T06:04:41.236414Z","iopub.execute_input":"2025-01-04T06:04:41.236686Z","iopub.status.idle":"2025-01-04T06:04:41.246375Z","shell.execute_reply.started":"2025-01-04T06:04:41.236662Z","shell.execute_reply":"2025-01-04T06:04:41.245742Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for estimator in estimators:\n    if estimator == 'cox':\n        pass\n    else:\n        params = {\n        'metric': 'rmse', \n        'random_state': 42,\n        'reg_alpha': 1.3502289561482839,\n        'reg_lambda': 7.699709830405624,\n        'colsample_bytree': 0.5,\n        'subsample': 0.7,\n        \"learning_rate\": 0.09286925251836978,\n        'max_depth': 10,\n        'num_leaves' : 232,\n        'min_child_samples': 125,\n        'cat_smooth' : 32\n    }\n\n        model = LGBMRegressor(\n            **params,\n            device=\"gpu\", \n            objective=\"regression\", \n            verbose=-1, \n            #early_stopping_rounds=25,\n        )\n        pipeline = SurvModel(model, estimator = estimator)\n        df_train, df_test = pipeline.preprocessing(train, test)\n        oof_lgb, preds_lgb,_ = pipeline.fit_predict(df_train,df_test)\n        pipeline.save_predictions(\"LightGBM_\"+estimator, df_train, df_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T06:04:41.247412Z","iopub.execute_input":"2025-01-04T06:04:41.247754Z","iopub.status.idle":"2025-01-04T06:05:06.807903Z","shell.execute_reply.started":"2025-01-04T06:04:41.247727Z","shell.execute_reply":"2025-01-04T06:05:06.807034Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for estimator in estimators:\n    if estimator == 'cox':\n        model = XGBRegressor(\n            device=\"cuda\",\n            max_depth=3,  \n            colsample_bytree=0.5,  \n            subsample=0.8,  \n            n_estimators=2000,  \n            learning_rate=0.02,  \n            enable_categorical = True,\n            objective = \"survival:cox\",\n            eval_metric = 'cox-nloglik'\n        )\n        \n    else:\n        \n        model = XGBRegressor(\n            device=\"cuda\",\n            max_depth=3,  \n            colsample_bytree=0.5,  \n            subsample=0.8,  \n            n_estimators=2000,  \n            learning_rate=0.02,  \n            enable_categorical=True,\n            verbose=0\n        )\n    pipeline = SurvModel(model, estimator = estimator)\n    df_train, df_test = pipeline.preprocessing(train, test)\n    oof_xgb, preds_xgb,_ = pipeline.fit_predict(df_train,df_test)\n    pipeline.save_predictions(\"XGBoost_\"+estimator, df_train, df_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T06:05:06.809222Z","iopub.execute_input":"2025-01-04T06:05:06.809632Z","iopub.status.idle":"2025-01-04T06:07:39.435833Z","shell.execute_reply.started":"2025-01-04T06:05:06.809591Z","shell.execute_reply":"2025-01-04T06:07:39.434852Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for estimator in estimators:\n    if estimator == 'cox':\n        model = CatBoostRegressor(loss_function = 'Cox', grow_policy='Lossguide',random_state = settings.seed)\n    else:\n        model = CatBoostRegressor(task_type = \"GPU\", grow_policy = 'Lossguide',random_state = settings.seed)\n    \n    pipeline = SurvModel(model, estimator = estimator)\n    df_train, df_test = pipeline.preprocessing(train, test)\n    oof_cat, preds_cat,_ = pipeline.fit_predict(df_train,df_test, True)\n    pipeline.save_predictions(\"Catboost_\"+estimator, df_train, df_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T06:07:39.437108Z","iopub.execute_input":"2025-01-04T06:07:39.437463Z","iopub.status.idle":"2025-01-04T06:25:36.683885Z","shell.execute_reply.started":"2025-01-04T06:07:39.437432Z","shell.execute_reply":"2025-01-04T06:25:36.682864Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Ensembling","metadata":{}},{"cell_type":"code","source":"esm = Ensembler(\"/kaggle/working/train\", \"/kaggle/working/test\")\nfinal_train, final_test = esm.concatenate_train_test(df_train)\nesm.ranking(final_train,final_test,df_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T06:25:36.685181Z","iopub.execute_input":"2025-01-04T06:25:36.685539Z","iopub.status.idle":"2025-01-04T06:25:38.594809Z","shell.execute_reply.started":"2025-01-04T06:25:36.685464Z","shell.execute_reply":"2025-01-04T06:25:38.593882Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"final_pred = np.zeros(len(final_test))\n\nfor col in final_test.columns:\n    if col not in settings.col_ignore:\n        final_pred += rankdata(final_test[col])\n\n\nfinal_pred","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T06:25:38.596175Z","iopub.execute_input":"2025-01-04T06:25:38.596567Z","iopub.status.idle":"2025-01-04T06:25:38.607864Z","shell.execute_reply.started":"2025-01-04T06:25:38.596526Z","shell.execute_reply":"2025-01-04T06:25:38.606951Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = pd.read_csv(settings.sub_link)\nsubmission['prediction'] = final_pred\n\nsubmission.to_csv(\"submission.csv\", index = False)\nsubmission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T06:25:38.608984Z","iopub.execute_input":"2025-01-04T06:25:38.609271Z","iopub.status.idle":"2025-01-04T06:25:38.627817Z","shell.execute_reply.started":"2025-01-04T06:25:38.609242Z","shell.execute_reply":"2025-01-04T06:25:38.627003Z"}},"outputs":[],"execution_count":null}]}